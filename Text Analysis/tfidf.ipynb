{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/susiesyli/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import log, col, row_number, split\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')\n",
    "# inaugural.words('2005-Bush.txt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/susiesyli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to all lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text inside square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove stopwords and words that are 3 letters or less (e.g. \"is\", \"the\", \"on\")\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words and len(word) > 3])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus: 10\n",
      "Number of unique words in each document: {'1985-Reagan.txt': 713, '1989-Bush.txt': 601, '1993-Clinton.txt': 484, '1997-Clinton.txt': 591, '2001-Bush.txt': 481, '2005-Bush.txt': 615, '2009-Obama.txt': 751, '2013-Obama.txt': 658, '2017-Trump.txt': 430, '2021-Biden.txt': 612}\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.inaugural.fileids()[-10:] # List with last 10 president speeches by title\n",
    "N = len(corpus) # Total number of documents in the corpus \n",
    "doc_lengths = {}\n",
    "all_words = [] # List of lists of words \n",
    "for doc in corpus: # For every speech \n",
    "        speech = \" \".join(inaugural.words(doc)) # Combine every word into a string\n",
    "        cleaned_text = clean_text(speech).split() # List of cleaned words\n",
    "        all_words.append(cleaned_text) # Clean text and split words into a list\n",
    "        doc_lengths[doc] = len(set(cleaned_text))\n",
    "\n",
    "print(\"Number of documents in corpus:\", N)\n",
    "print(\"Number of unique words in each document:\", doc_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Spark to find TF-IDF of every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext.getOrCreate()\n",
    "# rdd = sc.parallelize(all_words)\n",
    "# word_count = rdd.flatMap(lambda speech: set(speech)).map(lambda word: (word, 1))\n",
    "# word_count = word_count.reduceByKey(lambda x, y: x + y) # Aggregate by count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf(document):\n",
    "        word_count = Counter(document)\n",
    "        total = len(document)\n",
    "        tf = {word: count/total for word, count in word_count.items()}\n",
    "        return tf\n",
    "\n",
    "def find_tfidf(document, df):\n",
    "        tf = find_tf(document)\n",
    "        tfidf = {word: tf[word] * math.log(len(document)/df[word]) for word in tf}\n",
    "        return tfidf\n",
    "\n",
    "def top_tfidf_pres(tfidf, pres):\n",
    "        top20_words = []\n",
    "\n",
    "        pres_words = {}\n",
    "        \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'SparkSession' has no attribute 'getOrCreate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create SparkSession\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# value = \"tfidf\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# conf = SparkConf.setAppName(value).setMaster(\"local\")\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sc \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mgetOrCreate()\u001b[39m# conf=conf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m rdd \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mparallelize(all_words)\n\u001b[1;32m      6\u001b[0m word_count \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mflatMap(\u001b[39mlambda\u001b[39;00m speech: \u001b[39mset\u001b[39m(speech))\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m word: (word, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mreduceByKey(\u001b[39mlambda\u001b[39;00m x, y: x \u001b[39m+\u001b[39m y)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'SparkSession' has no attribute 'getOrCreate'"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "# value = \"tfidf\"\n",
    "# conf = SparkConf.setAppName(value).setMaster(\"local\")\n",
    "sc = SparkSession.getOrCreate()# conf=conf\n",
    "rdd = sc.parallelize(all_words)\n",
    "word_count = rdd.flatMap(lambda speech: set(speech)).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "df = word_count.collectAsMap()\n",
    "total_tfidf = [find_tfidf(document, df) for document in all_words]\n",
    "\n",
    "pres_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/13 01:32:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+-------------------+------------------+\n",
      "|      word| tf| df|                idf|            tf_idf|\n",
      "+----------+---+---+-------------------+------------------+\n",
      "|     story| 22|  4| 0.7884573603642703|17.346061928013945|\n",
      "|     human| 21|  4| 0.7884573603642703|16.557604567649676|\n",
      "|   freedom| 66|  8|0.20067069546215124|13.244265900501983|\n",
      "|      self| 10|  3| 1.0116009116784799|10.116009116784799|\n",
      "|    enough| 10|  3| 1.0116009116784799|10.116009116784799|\n",
      "|      days| 10|  3| 1.0116009116784799|10.116009116784799|\n",
      "|   century| 30|  7| 0.3184537311185346| 9.553611933556038|\n",
      "|generation| 21|  6| 0.4519851237430572| 9.491687598604202|\n",
      "|    dreams|  9|  3| 1.0116009116784799| 9.104408205106319|\n",
      "|      seen|  9|  3| 1.0116009116784799| 9.104408205106319|\n",
      "|      made| 20|  6| 0.4519851237430572| 9.039702474861144|\n",
      "|   journey| 19|  6| 0.4519851237430572| 8.587717351118087|\n",
      "|    breeze|  5|  1| 1.7047480922384253| 8.523740461192126|\n",
      "|      xand|  5|  1| 1.7047480922384253| 8.523740461192126|\n",
      "| sometimes|  8|  3| 1.0116009116784799| 8.092807293427839|\n",
      "|   nuclear|  8|  3| 1.0116009116784799| 8.092807293427839|\n",
      "|  everyone|  8|  3| 1.0116009116784799| 8.092807293427839|\n",
      "|   weapons|  8|  3| 1.0116009116784799| 8.092807293427839|\n",
      "|  requires|  8|  3| 1.0116009116784799| 8.092807293427839|\n",
      "|      jobs| 13|  5| 0.6061358035703155| 7.879765446414101|\n",
      "+----------+---+---+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"tfidf\").getOrCreate()\n",
    "\n",
    "# Combine all words in the corpus into one list\n",
    "all_corpus_words = [word for doc_words in all_words for word in doc_words]\n",
    "\n",
    "# Convert the combined corpus into DataFrame format\n",
    "corpus_words = spark.createDataFrame([(t, 1) for t in all_corpus_words], [\"word\", \"count\"])\n",
    "\n",
    "# Compute Term Frequency (TF) for the entire corpus\n",
    "tf = corpus_words.groupBy(\"word\").count().withColumnRenamed(\"count\", \"tf\")\n",
    "\n",
    "# Compute Document Frequency (DF). \n",
    "df_exploded = spark.createDataFrame([(doc, word) for doc, words in zip(corpus, all_words) for word in words], [\"document\", \"word\"])\n",
    "df_freq = df_exploded.dropDuplicates([\"word\", \"document\"]).groupBy(\"word\").count().withColumnRenamed(\"count\", \"df\")\n",
    "\n",
    "# Compute Inverse Document Frequency (IDF) for the entire corpus\n",
    "N = len(corpus)\n",
    "idf = df_freq.withColumn(\"idf\", log((N+1)/(col(\"df\")+1)))\n",
    "\n",
    "# Compute TF-IDF for the entire corpus\n",
    "tf_idf = tf.join(idf, on=\"word\", how=\"left\").withColumn(\"tf_idf\", col(\"tf\") * col(\"idf\"))\n",
    "\n",
    "# Display the results\n",
    "tf_idf.orderBy(col(\"tf_idf\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-check answer with scikit-learn's tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# # Create SparkSession\n",
    "# spark = SparkSession.builder.appName(\"tfidf_example\").getOrCreate()\n",
    "\n",
    "# # Convert your corpus data into DataFrame format\n",
    "# df_data = [(doc, \" \".join(words)) for doc, words in zip(corpus, all_words)]\n",
    "# df = spark.createDataFrame(df_data, [\"document\", \"words\"])\n",
    "\n",
    "# # Tokenize words (this will split the words into separate rows)\n",
    "# tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\n",
    "# tokensData = tokenizer.transform(df)\n",
    "\n",
    "# # Term Frequency\n",
    "# hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "# featurizedData = hashingTF.transform(tokensData)\n",
    "\n",
    "# # Inverse Document Frequency\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# # Extract top 20 words based on TF-IDF value for each document\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType, ArrayType\n",
    "# from pyspark.ml.linalg import SparseVector\n",
    "# import numpy as np\n",
    "\n",
    "# def extract_top_words(features):\n",
    "#     if isinstance(features, SparseVector):\n",
    "#         index_word = {int(v): k for k, v in hashingTF.indexOfTerm.items()}\n",
    "#         sorted_indices = np.argsort(features.toArray())[::-1][:20]\n",
    "#         return [index_word[idx] for idx in sorted_indices if idx in index_word]\n",
    "#     return []\n",
    "\n",
    "# udf_top_words = udf(extract_top_words, ArrayType(StringType()))\n",
    "# top_words_df = rescaledData.withColumn(\"top_20_words\", udf_top_words(\"features\"))\n",
    "# top_words_df.select(\"document\", \"top_20_words\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cc7239228f96f60021c839d45e73f0469089d31d4a341e8e753c0ae0a151559"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
